{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Load data\n",
    "train_df = pd.read_csv('tox21_train.csv')\n",
    "val_df = pd.read_csv('tox21_val.csv')\n",
    "test_df = pd.read_csv('tox21_test.csv')\n",
    "\n",
    "print(f\"Train set size: {train_df.shape}\")\n",
    "print(f\"Validation set size: {val_df.shape}\")\n",
    "print(f\"Test set size: {test_df.shape}\")\n",
    "\n",
    "# 2. Get task names\n",
    "tasks = train_df.columns[2:]  # Skip 'smiles' and 'mol_id'\n",
    "print(f\"Prediction tasks: {list(tasks)}\")\n",
    "\n",
    "# 3. Process SMILES strings\n",
    "# Create character-level tokenizer\n",
    "smiles_tokenizer = Tokenizer(char_level=True)\n",
    "smiles_tokenizer.fit_on_texts(train_df['smiles'])\n",
    "print(f\"Vocabulary size: {len(smiles_tokenizer.word_index) + 1}\")  # +1 for reserved index 0\n",
    "\n",
    "# Convert SMILES strings to sequences of integers\n",
    "train_sequences = smiles_tokenizer.texts_to_sequences(train_df['smiles'])\n",
    "val_sequences = smiles_tokenizer.texts_to_sequences(val_df['smiles'])\n",
    "test_sequences = smiles_tokenizer.texts_to_sequences(test_df['smiles'])\n",
    "\n",
    "# Get maximum sequence length for padding\n",
    "max_length = max([len(seq) for seq in train_sequences + val_sequences + test_sequences])\n",
    "print(f\"Max SMILES sequence length: {max_length}\")\n",
    "\n",
    "# Pad sequences to uniform length\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length)\n",
    "X_val = pad_sequences(val_sequences, maxlen=max_length)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# 4. Prepare target labels and handle missing values\n",
    "task_datasets = {}\n",
    "\n",
    "for task in tasks:\n",
    "    # Filter non-missing samples in each dataset\n",
    "    train_mask = ~train_df[task].isna()\n",
    "    task_X_train = X_train[train_mask]\n",
    "    task_Y_train = train_df.loc[train_mask, task].values.astype(float)\n",
    "\n",
    "    val_mask = ~val_df[task].isna()\n",
    "    task_X_val = X_val[val_mask]\n",
    "    task_Y_val = val_df.loc[val_mask, task].values.astype(float)\n",
    "\n",
    "    test_mask = ~test_df[task].isna()\n",
    "    task_X_test = X_test[test_mask]\n",
    "    task_Y_test = test_df.loc[test_mask, task].values.astype(float)\n",
    "\n",
    "    if len(task_Y_train) > 0 and len(task_Y_val) > 0 and len(task_Y_test) > 0:\n",
    "        # Check class balance\n",
    "        train_pos = np.sum(task_Y_train)\n",
    "        train_neg = len(task_Y_train) - train_pos\n",
    "        print(f\"{task}: Positive samples in train = {train_pos}, Negative = {train_neg}, Ratio = {train_pos/len(task_Y_train):.2f}\")\n",
    "\n",
    "        # Store datasets\n",
    "        task_datasets[task] = {\n",
    "            'X_train': task_X_train, 'Y_train': task_Y_train,\n",
    "            'X_val': task_X_val, 'Y_val': task_Y_val,\n",
    "            'X_test': task_X_test, 'Y_test': task_Y_test\n",
    "        }\n",
    "\n",
    "# 5. Build and train a model for each task\n",
    "def build_1d_cnn_model(vocab_size, max_length):\n",
    "    \"\"\"Build a 1D-CNN model for binary classification\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "        layers.SpatialDropout1D(0.2),\n",
    "\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        layers.Conv1D(filters=256, kernel_size=5, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "task_models = {}\n",
    "task_histories = {}\n",
    "test_results = {}\n",
    "\n",
    "vocab_size = len(smiles_tokenizer.word_index) + 1\n",
    "\n",
    "for task in task_datasets:\n",
    "    print(f\"\\nTraining model for {task}...\")\n",
    "\n",
    "    data = task_datasets[task]\n",
    "    model = build_1d_cnn_model(vocab_size, max_length)\n",
    "\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            patience=10, \n",
    "            restore_best_weights=True,\n",
    "            mode='max'\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        data['X_train'], data['Y_train'],\n",
    "        epochs=90,\n",
    "        batch_size=32,\n",
    "        validation_data=(data['X_val'], data['Y_val']),\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1,\n",
    "        class_weight={\n",
    "            0: 1.0,\n",
    "            1: np.sum(data['Y_train'] == 0) / np.sum(data['Y_train'] == 1) if np.sum(data['Y_train'] == 1) > 0 else 1.0\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(data['X_test']).ravel()\n",
    "    auc = roc_auc_score(data['Y_test'], y_pred)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(data['Y_test'], y_pred_binary)\n",
    "    fpr, tpr, _ = roc_curve(data['Y_test'], y_pred)\n",
    "\n",
    "    test_results[task] = {\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'roc': (fpr, tpr)\n",
    "    }\n",
    "\n",
    "    task_models[task] = model\n",
    "    task_histories[task] = history\n",
    "\n",
    "    print(f\"{task} Test AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 7. Visualization\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Task': list(test_results.keys()),\n",
    "    'AUC': [test_results[task]['auc'] for task in test_results],\n",
    "    'Accuracy': [test_results[task]['accuracy'] for task in test_results]\n",
    "})\n",
    "\n",
    "print(\"\\nTest set results:\")\n",
    "print(results_df)\n",
    "print(f\"Average Test AUC: {results_df['AUC'].mean():.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, task in enumerate(task_histories):\n",
    "    history = task_histories[task].history\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.plot(history['auc'], label='Train AUC')\n",
    "    plt.plot(history['val_auc'], label='Validation AUC')\n",
    "    plt.title(f'{task} AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot bar chart of AUCs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['Task'], results_df['AUC'])\n",
    "plt.axhline(y=results_df['AUC'].mean(), color='r', linestyle='-', label=f'Average AUC: {results_df[\"AUC\"].mean():.4f}')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Test AUC for Each Task')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, task in enumerate(test_results):\n",
    "    fpr, tpr = test_results[task]['roc']\n",
    "    auc = test_results[task]['auc']\n",
    "    plt.plot(fpr, tpr, label=f'{task} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('1D-CMM ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
